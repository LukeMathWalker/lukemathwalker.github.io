<!DOCTYPE html>
<html lang="en-uk">
	<head>
		<meta charset="utf-8">
		<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		
		<meta name="author" content="Luca Palmieri">
		<meta name="description" content="A personal blog focused on AI content and tutorials.">
		<meta name="generator" content="Hugo 0.31.1" />
		<title>Reinforcement Learning: a comprehensive introduction [Part 1] &middot; Travel journal of an AI adventure</title>
		<link rel="shortcut icon" href="http://lukemathwalker.github.io/images/favicon.ico">
		<link rel="stylesheet" href="http://lukemathwalker.github.io/css/style.css">
		<link rel="stylesheet" href="http://lukemathwalker.github.io/css/highlight.css">

		
		<link rel="stylesheet" href="http://lukemathwalker.github.io/css/font-awesome.min.css">
		

		
		<link href="http://lukemathwalker.github.io/index.xml" rel="alternate" type="application/rss+xml" title="Travel journal of an AI adventure" />
		

		
	</head>

    <body>
       <nav class="main-nav">
	
	
		<a href='http://lukemathwalker.github.io/'> <span class="arrow">←</span>Home</a>
	
	<a href='http://lukemathwalker.github.io/posts'>Archive</a>
	<a href='http://lukemathwalker.github.io/tags'>Tags</a>
	<a href='http://lukemathwalker.github.io/about'>About</a>

	

	
	<a class="cta" href="http://lukemathwalker.github.io/index.xml">Subscribe</a>
	
</nav>


        <section id="wrapper" class="post">
            <article>
                <header>
                    <h1>
                        Reinforcement Learning: a comprehensive introduction [Part 1]
                    </h1>
                    <h2 class="headline">
                    Dec 28, 2017 21:36
                    · 800 words
                    · 4 minute read
                      <span class="tags">
                      
                      </span>
                    </h2>
                </header>
                
                  
                    <div id="toc">
                      <nav id="TableOfContents">
<ul>
<li><a href="#notation-and-basic-definitions">Notation and basic definitions</a></li>
<li><a href="#policies">Policies</a>
<ul>
<li><a href="#decision-rules">Decision rules</a></li>
</ul></li>
<li><a href="#bellman-equation">Bellman equation</a></li>
<li><a href="#optimal-policies">Optimal policies</a></li>
</ul>
</nav>
                    </div>
                  
                
                <section id="post-body">
                    

<h1 id="notation-and-basic-definitions">Notation and basic definitions</h1>

<table>
<thead>
<tr>
<th align="center">Object</th>
<th align="center">Notation</th>
</tr>
</thead>

<tbody>
<tr>
<td align="center">State space</td>
<td align="center">$\mathcal{S}$</td>
</tr>

<tr>
<td align="center">State at time $t$</td>
<td align="center">$S_t$</td>
</tr>

<tr>
<td align="center">Available actions in state $s$</td>
<td align="center">$\mathcal{A}(s)$</td>
</tr>

<tr>
<td align="center">Action taken at time $t$</td>
<td align="center">$A_t$</td>
</tr>

<tr>
<td align="center">Reward space</td>
<td align="center">$\mathcal{R}$</td>
</tr>

<tr>
<td align="center">Reward after action $A_t$</td>
<td align="center">$R_{t+1}$</td>
</tr>
</tbody>
</table>

<ul>
<li>Let <code>$ a\in\mathcal{A}(s) $</code>, <code>$ r\in\mathcal{R} $</code> and <code>$s, s'\in\mathcal{S}$</code>. Then:
<code>$$ p(s', r\:|\: s,a):=\mathbb{P}\left(S_t=s', R_{t+1}=r\:|\:S_{t-1}=s, A_{t-1}=a\right) $$</code>
We have that (<em>probability measure</em>)
<code>$$
\sum_{s'\in\mathcal{S},\:r\in\mathcal{R}}p(s', r\:|\: s,a) = 1
$$</code>
<br></li>
<li>Let <code>$r(s,a)$</code> define the expected reward for a state-action pair, namely
<code>$$
r(s,a) := \mathbb{E}\left[R_t\:\middle|\:S_{t-1}=s,\: A_{t-1}=a\right]
$$</code>
Then
<code>$$
r(s,a) = \sum_{r\in\mathcal{R}} rp(r\:|\:s, a) = \sum_{r\in\mathcal{R}} r \sum_{s'\in\mathcal{S}} p(r, s'\:|\:s, a)
$$</code>
<br></li>
<li>Let <code>$r(s,a,s')$</code> define the expected reward for a state/action/next-state triple, namely
<code>$$
r(s,a,s') := \mathbb{E}\left[R_t\:\middle|\:S_{t-1}=s,\: A_{t-1}=a,\:S_{t}=s'\right]
$$</code>
Then
<code>$$
r(s,a,s') = \sum_{r\in\mathcal{R}} rp(r\:|\:s, a, s') = \sum_{r\in\mathcal{R}} r \sum_{s'\in\mathcal{S}} \frac{p(r, s'\:|\:s, a)}{p(s'\:|\:s,a)}
$$</code>
<br></li>
<li>The return at time $t$, $G_t$, is defined as follows:
<code>$$
G_t := \sum_{k=t+1}^{T}\gamma^{k-t-1}R_k
$$</code>
where <code>$\gamma\in[0, 1]$</code> is called <strong>discount rate</strong> and <code>$T\in\mathbb{R}\cup\{+\infty\}$</code>. <br>
We implicitly assume that if <code>$\gamma=1$</code> then <code>$T\neq +\infty$</code>, and vice versa. <br>
We can derive, just from the definition, an easy property of the return <code>$G_t$</code>:
<code>$$
G_t = \sum_{k=t+1}^{T}\gamma^{k-t-1}R_k = R_{t+1} + \gamma\sum_{k=t+2}^{T}\gamma^{k-t-2}R_k = R_{t+1} + \gamma G_{t+1}
$$</code>
Observe also that if <code>$T=+\infty$</code> and <code>$R_t$</code> is bounded for every <code>$t$</code> - namely, there exists <code>$M&gt;0$</code> such that <code>$|R_t|\leq M$</code> for all <code>$t$</code> - we still have that <code>$G_t$</code> is finite. In fact:
<code>$$
|G_t| \leq \sum_{k=t+1}^{+\infty}\left|\gamma^{k-t-1}R_k\right| = \sum_{k=t+1}^{+\infty}\gamma^{k-t-1}\left|R_k\right| \leq M \sum_{k=t+1}^{+\infty}\gamma^{k-t-1} = \frac{M}{1-\gamma} &lt; +\infty
$$</code>
<br></li>
</ul>

<h1 id="policies">Policies</h1>

<h2 id="decision-rules">Decision rules</h2>

<p>A <strong>randomized history-dependent decision rule</strong> <code>$\pi$</code> is a mapping between the history of our system up to the current state and the probabilities of selecting each possible action. <br>
What is the history of the system? Nothing more than the sequence of past actions and states (we are not considering decision rules depending on past rewards). <br>
We shall denote the history of our system up to time <code>$t$</code> as
<code>$$h_t=(s_1, a_1, s_2, \dots, a_{t-1}, s_t) \quad\quad t\in\{1, 2, \dots\}$$</code>
Using a little bit of recursion we can get a much more compact notation for our system history:
<code>$$
h_t=(h_{t-1}, a_t, s_t) \quad\quad\quad\quad t\in\{1, 2, \dots\}
$$</code>
We can thus define the set of histories as
<code>$$
H_t=\prod_1^t (S\times A) \quad\quad t\in\{1, 2, \dots\}
$$</code>
or, equivalenty,
<code>$$
H_t=H_{t-1}\times S \times A \quad\quad t\in\{1, 2, \dots\}
$$</code>
We can thus formally denote a history-dependent decision rule <code>$\pi$</code> as a function
<code>$$
\pi: H_t \to \mathcal{P}(A)
$$</code>
We shall denote by <code>$b_{\pi}(a\,|\,h_t)$</code> or <code>$b_{\pi(h_t)}(a)$</code> the probability of taking action <code>$a$</code> if our system history up to time <code>$t$</code> is equal to <code>$h_t$</code>. <br></p>

<p>We can also consider a <strong>deterministic history-dependent decision rule</strong> <code>$\pi$</code>, which can be seen as a function:
<code>$$
\pi: H_t \to A
$$</code>
or, equivalently, as a randomized history-dependent decision rule such that for each <code>$h_t$</code> there exists a unique <code>$a\in A$</code> with <code>$b_{\pi(h_t)}(a)=1$</code>.<br></p>

<p>If we denote the set of randomized history-dependent decision rules as <code>$\Pi^{HR}$</code> and the set of deterministic history-dependent decision rule as <code>$\Pi^{HD}$</code> we clearly have
<code>$$
\Pi^{HD} \subset \Pi^{HR} 
$$</code></p>

<p>The value of a state <code>$s$</code> under a policy <code>$\pi$</code> is the expected return when starting in $s$ and following $\pi$ thereafter. We shall denote it by <code>$v_\pi(s)$</code>. Formally:
<code>$$
v_\pi(s):= \mathbb{E}_{\pi}\left[G_t\:\middle|\:S_t=s\right]
$$</code>
<code>$v_\pi$</code> is also called the <strong>state-value function for policy <code>$\pi$</code></strong>.
<br></p>

<p>The value of taking action $a$ in state $s$ under a policy <code>$\pi$</code> is the expected return when starting in <code>$s$</code>, taking action $a$ and therefore following <code>$\pi$</code>. We shall denote it by <code>$q_\pi(s, a)$</code>. Formally:
<code>$$
q_\pi(s, a):= \mathbb{E}_{\pi}\left[G_t\:\middle|\:S_t=s,\,A_t=a\right]
$$</code>
<code>$q_\pi(s, a)$</code> is also called the <strong>action-value function for policy <code>$\pi$</code></strong>.
<br></p>

<h1 id="bellman-equation">Bellman equation</h1>

<p>The state-value function for a policy <code>$\pi$</code> satisfies a recursive relationship similar (in spirit) to the one satisfied by the return <code>$G_t$</code>. We have
<code>$$
\begin{align}
v_\pi(s) &amp;= \mathbb{E}_{\pi}\left[G_t\:\middle|\:S_t=s\right]= \\
&amp;= \mathbb{E}_{\pi}\left[R_{t+1} +\gamma G_{t+1}\:\middle|\:S_t=s\right]= \\
&amp;= \mathbb{E}_{\pi}\left[R_{t+1}\:\middle|\:S_t=s\right] +\gamma \mathbb{E}_{\pi}\left[G_{t+1}\:\middle|\:S_t=s\right]= \\
&amp;= \sum_{r\in\mathcal{R}}r\,\mathbb{P}\left( R_{t+1}=r\:\middle|\: S_{t}=s \right) +\gamma \mathbb{E}_{\pi}\left[G_{t+1}\:\middle|\:S_t=s\right]
\end{align}
$$</code>
But
<code>$$
\begin{align}
\mathbb{P}\left( R_{t+1}=r\:\middle|\: S_{t}=s \right) &amp;=
\sum_{s'\in\mathcal{S}}\mathbb{P}\left( R_{t+1}=r,\, S_{t+1}=s'\:\middle|\: S_{t}=s \right) = \\
&amp;= \sum_{s'\in\mathcal{S}}\sum_{a\in\mathcal{A(s)}}\mathbb{P}\left( R_{t+1}=r,\, S_{t+1}=s'\:\middle|\: S_{t}=s, A_{t}=a \right) \mathbb{P}\left( A_{t}=a\:\middle|\: S_{t}=s \right)= \\
&amp;= \sum_{s'\in\mathcal{S}}\sum_{a\in\mathcal{A(s)}} p(r,\,s'\:|\:s,\,a)\pi(a\:|\:s)
\end{align}
$$</code>
Thus
<code>$$
v_\pi(s) = \sum_{r\in\mathcal{R}}\sum_{s'\in\mathcal{S}}\sum_{a\in\mathcal{A(s)}} \left[r\,p(r,\,s'\:|\:s,\,a)\pi(a\:|\:s)\right] +\gamma \mathbb{E}_{\pi}\left[G_{t+1}\:\middle|\:S_t=s\right]
$$</code>
We can exchange the sum order and, keeping in mind that <code>$\sum_{r\in\mathcal{R}}\sum_{s'\in\mathcal{S}}p(r,\,s'\:|\:s,\,a)=1$</code> and <code>$\sum_{a\in\mathcal{A(s)}}\pi(a\:|\:s)=1$</code>, we get
<code>$$
\begin{align}
v_\pi(s) &amp;= \sum_{a\in\mathcal{A(s)}}\pi(a\:|\:s)\sum_{r\in\mathcal{R}}\sum_{s'\in\mathcal{S}} p(r,\,s'\:|\:s,\,a)\bigg[r+\gamma \mathbb{E}_{\pi}\left[G_{t+1}\:\middle|\:S_t=s\right]\bigg]
\end{align}
$$</code>
This is called <strong>Bellman equation for <code>$v_\pi$</code></strong>.</p>

<h1 id="optimal-policies">Optimal policies</h1>

<p>Let <code>$v_*$</code> be defined as follows:
<code>$$v_*(s) := \max_{\pi \: \text{policy}} v_\pi(s)$$</code>
for all <code>$s\in\mathcal{S}$</code>.<br>
<code>$v_*$</code> is called <strong>optimal state-value function</strong>.
<br></p>

<p>Let <code>$q_*$</code> be defined as follows:
<code>$$q_*(s, a) := \max_{\pi \: \text{policy}} q_\pi(s, a)$$</code>
for all <code>$s\in\mathcal{S}$</code>.<br>
<code>$q_*$</code> is called <strong>optimal action-value function</strong>.
<br></p>

<p>A policy <code>$\pi$</code> is said to be <em>better than or equal</em> to a policy <code>$\pi'$</code> if <code>$v_\pi(s)\geq v_{\pi'}(s)$ for all $s\in\mathcal{S}$</code>. <br>
This is a <strong>partial order</strong> relationship on the set of policies: not all policies can be compared! <br></p>

                </section>
            </article>

            
                <a class="twitter" href="https://twitter.com/intent/tweet?text=http%3a%2f%2flukemathwalker.github.io%2fposts%2fmy-first-post%2f - Reinforcement%20Learning%3a%20a%20comprehensive%20introduction%20%5bPart%201%5d "><span class="icon-twitter"> tweet</span></a>

<a class="facebook" href="#" onclick="
    window.open(
      'https://www.facebook.com/sharer/sharer.php?u='+encodeURIComponent(location.href),
      'facebook-share-dialog',
      'width=626,height=436');
    return false;"><span class="icon-facebook-rect"> Share</span>
</a>

            

            

            

            <footer id="footer">
    
        <div id="social">

	
	
    <a class="symbol" href="https://www.github.com/LukeMathWalker">
        <i class="fa fa-github-square"></i>
    </a>
    
    <a class="symbol" href="https://www.linkedin.com/in/luca-palmieri/">
        <i class="fa fa-linkedin-square"></i>
    </a>
    


</div>

    
    <p class="small">
    
       © Copyright 2017 <i class="fa fa-heart" aria-hidden="true"></i> Luca Palmieri
    
    </p>
    <p class="small">
        Powered by <a href="http://www.gohugo.io/">Hugo</a> Theme By <a href="https://github.com/nodejh/hugo-theme-cactus-plus">nodejh</a>
    </p>
</footer>

        </section>

        <script src="http://lukemathwalker.github.io/js/jquery-2.2.4.min.js"></script>
<script src="http://lukemathwalker.github.io/js/main.js"></script>
<script src="http://lukemathwalker.github.io/js/highlight.min.js"></script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "all" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({

  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>

<script>hljs.initHighlightingOnLoad();</script>







    </body>
</html>
