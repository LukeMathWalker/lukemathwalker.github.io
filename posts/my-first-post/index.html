<!DOCTYPE html>
<html lang="en-uk">
	<head>
		<meta charset="utf-8">
		<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		
		<meta name="author" content="Luca Palmieri">
		<meta name="description" content="A personal blog focused on AI content and tutorials.">
		<meta name="generator" content="Hugo 0.31.1" />
		<title>Reinforcement Learning: a comprehensive introduction [Part 1] &middot; Travel journal of an AI adventure</title>
		<link rel="shortcut icon" href="http://lukemathwalker.github.io/images/favicon.ico">
		<link rel="stylesheet" href="http://lukemathwalker.github.io/css/style.css">
		<link rel="stylesheet" href="http://lukemathwalker.github.io/css/highlight.css">

		
		<link rel="stylesheet" href="http://lukemathwalker.github.io/css/font-awesome.min.css">
		

		
		<link href="http://lukemathwalker.github.io/index.xml" rel="alternate" type="application/rss+xml" title="Travel journal of an AI adventure" />
		

		
	</head>

    <body>
       <nav class="main-nav">
	
	
		<a href='http://lukemathwalker.github.io/'> <span class="arrow">←</span>Home</a>
	
	<a href='http://lukemathwalker.github.io/posts'>Archive</a>
	<a href='http://lukemathwalker.github.io/tags'>Tags</a>
	<a href='http://lukemathwalker.github.io/about'>About</a>

	

	
	<a class="cta" href="http://lukemathwalker.github.io/index.xml">Subscribe</a>
	
</nav>


        <section id="wrapper" class="post">
            <article>
                <header>
                    <h1>
                        Reinforcement Learning: a comprehensive introduction [Part 1]
                    </h1>
                    <h2 class="headline">
                    Dec 28, 2017 21:36
                    · 1532 words
                    · 8 minute read
                      <span class="tags">
                      
                      </span>
                    </h2>
                </header>
                
                  
                    <div id="toc">
                      <nav id="TableOfContents">
<ul>
<li><a href="#policies">Policies</a>
<ul>
<li><a href="#decision-rules">Decision rules</a></li>
<li><a href="#policies-1">Policies</a></li>
<li><a href="#returns">Returns</a></li>
<li><a href="#value-functions-and-optimality">Value functions and Optimality</a></li>
</ul></li>
<li><a href="#bellman-equation">Bellman equation</a></li>
<li><a href="#optimal-policies">Optimal policies</a></li>
</ul>
</nav>
                    </div>
                  
                
                <section id="post-body">
                    

<!---
# Notation and basic definitions

| Object | Notation   |
|:------:|:------:|
|   State space  | $\mathcal{S}$|
|   State at time $t$  | $S_t$|
|   Available actions in state $s$  | $\mathcal{A}(s)$|
|   Action taken at time $t$  | $A_t$|
|   Reward space  | $\mathcal{R}$|
|   Reward after action $A_t$  | $R_{t+1}$|

- Let `$ a\in\mathcal{A}(s) $`, `$ r\in\mathcal{R} $` and `$s, s'\in\mathcal{S}$`. Then:
`$$ p(s', r\:|\: s,a):=\mathbb{P}\left(S_t=s', R_{t+1}=r\:|\:S_{t-1}=s, A_{t-1}=a\right) $$`
We have that (*probability measure*)
`$$
\sum_{s'\in\mathcal{S},\:r\in\mathcal{R}}p(s', r\:|\: s,a) = 1
$$`
<br>
- Let `$r(s,a)$` define the expected reward for a state-action pair, namely
`$$
r(s,a) := \mathbb{E}\left[R_t\:\middle|\:S_{t-1}=s,\: A_{t-1}=a\right]
$$`
Then
`$$
r(s,a) = \sum_{r\in\mathcal{R}} rp(r\:|\:s, a) = \sum_{r\in\mathcal{R}} r \sum_{s'\in\mathcal{S}} p(r, s'\:|\:s, a)
$$`
<br>
- Let `$r(s,a,s')$` define the expected reward for a state/action/next-state triple, namely
`$$
r(s,a,s') := \mathbb{E}\left[R_t\:\middle|\:S_{t-1}=s,\: A_{t-1}=a,\:S_{t}=s'\right]
$$`
Then
`$$
r(s,a,s') = \sum_{r\in\mathcal{R}} rp(r\:|\:s, a, s') = \sum_{r\in\mathcal{R}} r \sum_{s'\in\mathcal{S}} \frac{p(r, s'\:|\:s, a)}{p(s'\:|\:s,a)}
$$`
<br>
-->

<h1 id="policies">Policies</h1>

<h2 id="decision-rules">Decision rules</h2>

<p>A <strong>randomized history-dependent decision rule</strong> <code>$d_t$</code> is a mapping between the history of our system up to the current state <code>$t$</code> and the probability of selecting each possible action. <br>
What is the history of the system? Nothing more than the sequence of past actions and states, assuming that past rewards do not influence our future behaviour. <br>
We shall denote the history of our system up to time <code>$t$</code> as
<code>$$h_t=(s_1, a_1, s_2, \dots, a_{t-1}, s_t) \quad\quad t\in\{1, 2, \dots\}$$</code>
or, using a little bit of recursion,
<code>$$
h_t=(h_{t-1}, a_t, s_t) \quad\quad\quad\quad t\in\{1, 2, \dots\}
$$</code>
The set of histories can be defined as
<code>$$
H_t=\prod_1^t (S\times A) \quad\quad t\in\{1, 2, \dots\}
$$</code>
or, equivalenty,
<code>$$
H_t=H_{t-1}\times S \times A \quad\quad t\in\{1, 2, \dots\}
$$</code>
We can thus formalize the concept of history-dependent decision rule as a function
<code>$$
d_t: H_t \to \mathcal{P}(A)
$$</code>
where <code>$\mathcal{P}(A)$</code> denotes the collection of probability distributions over the set of actions <code>$A$</code>. <br>
We shall denote by <code>$b_{d_t}(a\,|\,h_t)$</code> or <code>$b_{d_t(h_t)}(a)$</code> the probability of taking action <code>$a$</code> if our system history up to time <code>$t$</code> is equal to <code>$h_t$</code>. <br></p>

<p>We can also consider a <strong>deterministic history-dependent decision rule</strong> <code>$d_t$</code>, which can be seen as a function:
<code>$$
d_t: H_t \to A
$$</code>
or, equivalently, as a randomized history-dependent decision rule such that for each <code>$h_t$</code> there exists a unique <code>$a\in A$</code> with <code>$b_{d_t(h_t)}(a)=1$</code> and <code>$b_{d_t(h_t)}(a')=0$</code> for all <code>$a'\neq a$</code>.<br></p>

<p>If we denote the set of randomized history-dependent decision rules at time <code>$t$</code> as <code>$D_t^{HR}$</code> and the set of deterministic history-dependent decision rule as <code>$D_t^{HD}$</code> we clearly have, according to our last observation,
<code>$$
D_t^{HD} \subset D_t^{HR}
$$</code></p>

<p>Considering the whole history of the system up to the current state is certainly the most general viewpoint we can choose before committing to a certain action. Nonetheless it is computationally very expensive to keep track of a function whose input dimension grows exponentially in time: consider, for example, a state space <code>$\mathcal{S}$</code> with five elements equipped with an action set <code>$\mathcal{A}$</code> composed of three choices. <br>
For <code>$t=1$</code> we have 5 elements in <code>$H_1$</code> (we take <code>$t=1$</code> as starting state). <br>
For <code>$t=2$</code> we have 5x3x5=75 elements in <code>$H_2$</code>. <br>
For an arbitrary <code>$t\in\mathbb{N}$</code> we have
<code>$$
|H_t| = 5^t 3^{t-1}
$$</code>
which, you surely agree with me, is going to become problematic soon enough, even if the system is ridiculously small. <br></p>

<p>A much more convenient class of decision rules is the collection of <strong>randomized markovian decision rules</strong>, which we denote by <code>$D^{MR}$</code>: the action choice is chosen randomly from a probability distribution depending only on the <em>current</em> state of the system (no <code>$t$</code> subscript this time!).<br>
This can be stated formally saying that every <code>$d\in D^{MR}$</code> if a function from <code>$\mathcal{S}$</code> to <code>$\mathcal{P}(\mathcal{A})$</code>. <br></p>

<p>Just as we did for history-dependent decision rules we can identify the subset of <strong>deterministic markovian decision rules</strong>, which are simply functions from <code>$\mathcal{S}$</code> to <code>$\mathcal{A}$</code>. We denote them by <code>$D^{MD}$</code>.<br></p>

<p>We can thus provide chains of inclusions between these families of decision rules, in increasing order of generality:
<code>$$
\begin{align}
D^{MD} &amp;\subset D_t^{HD} \subset D_t^{HR} \\
D^{MD} &amp;\subset D^{MR} \subset D_t^{HR} \\
\end{align}
$$</code></p>

<p>The computational gain is evident: a deterministic markovian decision rule <code>$d$</code> only requires <code>$2|\mathcal{S}|$</code> memory slots - we just need to keep track of the action index associated to each possible state of the system. <br>
Randomized markovian decision rules are slightly more expensive - they require <code>$|\mathcal{S}||\mathcal{A}|$</code> memory slots, but they are still a feasible choice. <br></p>

<h2 id="policies-1">Policies</h2>

<p>Decision rules are only a piece of the puzzle - how to choose the next action to take considering the current system history. <br>
If we want to specify the system behaviour from the initial state onwards we need to provide a <strong>sequence</strong> of decision rules - a so-called <strong>policy</strong>. <br></p>

<p>We can formally describe a policy as an (infinite) ordered sequence of decision rules indexed by the time <code>$t$</code>:
<code>$$
\pi = (d_1, d_2, \dots)
$$</code>
or, using a more compact notation,
<code>$$
\pi \in \prod_{t=1}^{+\infty} D_t^K
$$</code>
where <code>$K=\{MD, MR, HD, HR\}$</code>. <br>
We shall always assume that policies are composed of decision rules of the same type. We thus have:</p>

<ul>
<li><strong>randomized history-dependent policies</strong>, <code>$\Pi^{HR}$</code>;</li>
<li><strong>deterministic history-dependent policies</strong>, <code>$\Pi^{HD}$</code>;</li>
<li><strong>randomized markovian policies</strong>, <code>$\Pi^{MR}$</code>;</li>
<li><strong>deterministic markovian policies</strong>, <code>$\Pi^{MD}$</code>.</li>
</ul>

<p>If storing a <em>history-dependent decision rule</em> was troublesome, just meditate for a second on the problem of storing a <em>history-dependent policy</em>! <br>
Unfortunately even a deterministic markovian policy is fearsome to store if we are considering a potentially infinite number of time steps. <br></p>

<p>This is why we introduce a new family of policies: a policy <code>$\pi$</code> is said to be a <strong>randomized stationary policy</strong> if <code>$d_t=d$</code> for all <code>$t\in\mathbb{N}$</code> with <code>$d\in D^{MR}$</code>, i.e.
<code>$$
\pi = (d, d, \dots)
$$</code>
We use <code>$d^\infty$</code> as a shortcut for <code>$(d, d, \dots)$</code>. <br>
The set of randomized stationary policies is denoted by <code>$\Pi^{SR}$</code>, while we use <code>$\Pi^{SD}$</code> for the subset of <strong>deterministic stationary policies</strong>. <br></p>

<p>With stationary polies the memory footprint goes back to the one computed for markovian decision rules, which is more than desirable. Nonetheless we must ask ourselves a couple of important questions:</p>

<ul>
<li>Can we achieve the same rewards using stationary policies?</li>
<li>What hypotheses do we need to be sure that an <strong>optimal</strong> policy for our system can be found in the family of stationary policies?</li>
</ul>

<p>Before engaging in this interesting discussion we should define what we mean by <strong>optimal</strong> when we are speaking of policies.</p>

<h2 id="returns">Returns</h2>

<p>How do we measure the performances of a policy? <br>
In a Reinforcement Learning context, every action is followed by a reward (positive or negative). To measure the overall performance of a policy we need to combine all these rewards together - this is usually called <strong>return</strong> in the literature. <br>
We can craft arbitrarily complex return criteria, but in this blog post we are going to stick to most common one: discounted cumulative return. <br>
If <code>$R_{t+1}$</code> denotes the reward obtained after action <code>$A_t$</code> we can define the discounted cumulative return at time <code>$t$</code>, <code>$G_t$</code>, as
<code>$$
G_t := \sum_{k=t+1}^{T}\gamma^{k-t-1}R_k
$$</code>
where <code>$\gamma\in[0, 1]$</code> is called <strong>discount rate</strong> and <code>$T\in\mathbb{R}\cup\{+\infty\}$</code>. <br>
If we are dealing with a finite time horizon <code>$T$</code> then we can simply set <code>$\gamma=1$</code> and consider the cumulative return, i.e.
<code>$$
G_t := \sum_{k=t+1}^{T}R_k
$$</code>
But this setting becomes problematic if the time horizon is (potentially) infinite. <br> Introducing a discount rate <code>$\gamma\in[0,1)$</code> allows us to associate a bigger weight to closer rewards as well as ensuring that the overall return <code>$G_t$</code> remains finite if our rewards are bounded:
<code>$$
|G_t| \leq \sum_{k=t+1}^{+\infty}\left|\gamma^{k-t-1}R_k\right| = \sum_{k=t+1}^{+\infty}\gamma^{k-t-1}\left|R_k\right| \leq M \sum_{k=t+1}^{+\infty}\gamma^{k-t-1} = \frac{M}{1-\gamma} &lt; +\infty
$$</code>
assuming that there exists <code>$M&gt;0$</code> such that <code>$|R_t|\leq M$</code> for all <code>$t$</code>. <br>
We implicitly assume that if <code>$\gamma=1$</code> then <code>$T\neq +\infty$</code>, and vice versa. <br>
<!---
We can derive, just from the definition, an easy property of the return `$G_t$`:
`$$
G_t = \sum_{k=t+1}^{T}\gamma^{k-t-1}R_k = R_{t+1} + \gamma\sum_{k=t+2}^{T}\gamma^{k-t-2}R_k = R_{t+1} + \gamma G_{t+1}
$$`
--></p>

<h2 id="value-functions-and-optimality">Value functions and Optimality</h2>

<p>We can finally define what a policy is <em>worth</em> by looking at the expected return when following that sequence of decision rules! <br>
We encode this concept into the so-called <strong>value function</strong>. <br></p>

<p>The value of a history <code>$h_t$</code> under a policy <code>$\pi$</code> is the expected return when starting in <code>$h_t$</code> and following <code>$\pi$</code> thereafter. We shall denote it by <code>$v_\pi(h_t)$</code>. Formally:
<code>$$
v^t_\pi(h_t):= \mathbb{E}_{\pi}\left[G_t\:\middle|\:H_t=h_t\right]
$$</code>
<code>$v^t_\pi$</code> is called the <strong>history-value function for policy <code>$\pi$</code> at time <code>$t$</code></strong>.
<br></p>

<p>We can thus affirm that a policy <code>$\pi$</code> is better than a policy <code>$\pi'$</code> if
<code>$$
v^1_\pi(s) &gt; v^1_{\pi'}(s) \quad \forall s\in\mathcal{S}
$$</code>
where <code>$s$</code> is the initial state of our system. <br></p>

<p>A policy <code>$\pi$</code> is said to be <strong>optimal</strong> if
<code>$$
v^1_\pi(s) &gt; v^1_{\pi'}(s) \quad \forall s\in\mathcal{S}
$$</code>
for all <code>$\pi'\in\Pi^{HR}$</code> [remember that randomized history-dependent policies are the most general class!].</p>

<p>It is important to remark that <code>$v_{(\cdot)}$</code> is an <em>expected value</em> - even if a policy <code>$\pi$</code> is better <em>on average</em> than a policy <code>$\pi'$</code> starting from a state <code>$s$</code> it might happen than an experiment using policy <code>$\pi$</code> produces a higher return compared to an experiment using policy <code>$\pi'$</code>: stochasticity baby! <br>
But we are going to neglect these issues for the moment, focusing on the value function.</p>

<!---
The value of taking action $a$ in state $s$ under a policy `$\pi$` is the expected return when starting in `$s$`, taking action $a$ and therefore following `$\pi$`. We shall denote it by `$q_\pi(s, a)$`. Formally:
`$$
q_\pi(s, a):= \mathbb{E}_{\pi}\left[G_t\:\middle|\:S_t=s,\,A_t=a\right]
$$`
`$q_\pi(s, a)$` is also called the **action-value function for policy `$\pi$`**.
<br>
-->

<h1 id="bellman-equation">Bellman equation</h1>

<p>The state-value function for a policy <code>$\pi$</code> satisfies a recursive relationship similar (in spirit) to the one satisfied by the return <code>$G_t$</code>. We have
<code>$$
\begin{align}
v_\pi(s) &amp;= \mathbb{E}_{\pi}\left[G_t\:\middle|\:S_t=s\right]= \\
&amp;= \mathbb{E}_{\pi}\left[R_{t+1} +\gamma G_{t+1}\:\middle|\:S_t=s\right]= \\
&amp;= \mathbb{E}_{\pi}\left[R_{t+1}\:\middle|\:S_t=s\right] +\gamma \mathbb{E}_{\pi}\left[G_{t+1}\:\middle|\:S_t=s\right]= \\
&amp;= \sum_{r\in\mathcal{R}}r\,\mathbb{P}\left( R_{t+1}=r\:\middle|\: S_{t}=s \right) +\gamma \mathbb{E}_{\pi}\left[G_{t+1}\:\middle|\:S_t=s\right]
\end{align}
$$</code>
But
<code>$$
\begin{align}
\mathbb{P}\left( R_{t+1}=r\:\middle|\: S_{t}=s \right) &amp;=
\sum_{s'\in\mathcal{S}}\mathbb{P}\left( R_{t+1}=r,\, S_{t+1}=s'\:\middle|\: S_{t}=s \right) = \\
&amp;= \sum_{s'\in\mathcal{S}}\sum_{a\in\mathcal{A(s)}}\mathbb{P}\left( R_{t+1}=r,\, S_{t+1}=s'\:\middle|\: S_{t}=s, A_{t}=a \right) \mathbb{P}\left( A_{t}=a\:\middle|\: S_{t}=s \right)= \\
&amp;= \sum_{s'\in\mathcal{S}}\sum_{a\in\mathcal{A(s)}} p(r,\,s'\:|\:s,\,a)\pi(a\:|\:s)
\end{align}
$$</code>
Thus
<code>$$
v_\pi(s) = \sum_{r\in\mathcal{R}}\sum_{s'\in\mathcal{S}}\sum_{a\in\mathcal{A(s)}} \left[r\,p(r,\,s'\:|\:s,\,a)\pi(a\:|\:s)\right] +\gamma \mathbb{E}_{\pi}\left[G_{t+1}\:\middle|\:S_t=s\right]
$$</code>
We can exchange the sum order and, keeping in mind that <code>$\sum_{r\in\mathcal{R}}\sum_{s'\in\mathcal{S}}p(r,\,s'\:|\:s,\,a)=1$</code> and <code>$\sum_{a\in\mathcal{A(s)}}\pi(a\:|\:s)=1$</code>, we get
<code>$$
\begin{align}
v_\pi(s) &amp;= \sum_{a\in\mathcal{A(s)}}\pi(a\:|\:s)\sum_{r\in\mathcal{R}}\sum_{s'\in\mathcal{S}} p(r,\,s'\:|\:s,\,a)\bigg[r+\gamma \mathbb{E}_{\pi}\left[G_{t+1}\:\middle|\:S_t=s\right]\bigg]
\end{align}
$$</code>
This is called <strong>Bellman equation for <code>$v_\pi$</code></strong>.</p>

<h1 id="optimal-policies">Optimal policies</h1>

<p>Let <code>$v_*$</code> be defined as follows:
<code>$$v_*(s) := \max_{\pi \: \text{policy}} v_\pi(s)$$</code>
for all <code>$s\in\mathcal{S}$</code>.<br>
<code>$v_*$</code> is called <strong>optimal state-value function</strong>.
<br></p>

<p>Let <code>$q_*$</code> be defined as follows:
<code>$$q_*(s, a) := \max_{\pi \: \text{policy}} q_\pi(s, a)$$</code>
for all <code>$s\in\mathcal{S}$</code>.<br>
<code>$q_*$</code> is called <strong>optimal action-value function</strong>.
<br></p>

<p>A policy <code>$\pi$</code> is said to be <em>better than or equal</em> to a policy <code>$\pi'$</code> if <code>$v_\pi(s)\geq v_{\pi'}(s)$ for all $s\in\mathcal{S}$</code>. <br>
This is a <strong>partial order</strong> relationship on the set of policies: not all policies can be compared! <br></p>

                </section>
            </article>

            
                <a class="twitter" href="https://twitter.com/intent/tweet?text=http%3a%2f%2flukemathwalker.github.io%2fposts%2fmy-first-post%2f - Reinforcement%20Learning%3a%20a%20comprehensive%20introduction%20%5bPart%201%5d "><span class="icon-twitter"> tweet</span></a>

<a class="facebook" href="#" onclick="
    window.open(
      'https://www.facebook.com/sharer/sharer.php?u='+encodeURIComponent(location.href),
      'facebook-share-dialog',
      'width=626,height=436');
    return false;"><span class="icon-facebook-rect"> Share</span>
</a>

            

            

            

            <footer id="footer">
    
        <div id="social">

	
	
    <a class="symbol" href="https://www.github.com/LukeMathWalker">
        <i class="fa fa-github-square"></i>
    </a>
    
    <a class="symbol" href="https://www.linkedin.com/in/luca-palmieri/">
        <i class="fa fa-linkedin-square"></i>
    </a>
    


</div>

    
    <p class="small">
    
       © Copyright 2017 <i class="fa fa-heart" aria-hidden="true"></i> Luca Palmieri
    
    </p>
    <p class="small">
        Powered by <a href="http://www.gohugo.io/">Hugo</a> Theme By <a href="https://github.com/nodejh/hugo-theme-cactus-plus">nodejh</a>
    </p>
</footer>

        </section>

        <script src="http://lukemathwalker.github.io/js/jquery-2.2.4.min.js"></script>
<script src="http://lukemathwalker.github.io/js/main.js"></script>
<script src="http://lukemathwalker.github.io/js/highlight.min.js"></script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "all" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({

  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>

<script>hljs.initHighlightingOnLoad();</script>







    </body>
</html>
